{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "residential-jesus",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import cllm_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "altered-blood",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['w', 'm', 'q', 't', 'b', 'x', 'u', 'c', 'i', 's', 'z', 'o', 'p', 'y', 'd', 'v', 'l', 'r', 'h', 'f', 'k', 'e', 'g', 'a', '\\n', 'j', 'n']\n",
      "共计有19909个字符，唯一字符有27个\n"
     ]
    }
   ],
   "source": [
    "#获取名称\n",
    "data = open(\"dinos.txt\",\"r\").read()\n",
    "\n",
    "#转化为小写\n",
    "data = data.lower()\n",
    "\n",
    "#转化为不重复的元素列表\n",
    "chars = list(set(data))\n",
    "\n",
    "#获取大小信息\n",
    "data_size, vocab_size = len(data), len(chars)\n",
    "\n",
    "print(chars)\n",
    "print(\"共计有%d个字符，唯一字符有%d个\"%(data_size,vocab_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "phantom-marshall",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'\\n': 0, 'a': 1, 'b': 2, 'c': 3, 'd': 4, 'e': 5, 'f': 6, 'g': 7, 'h': 8, 'i': 9, 'j': 10, 'k': 11, 'l': 12, 'm': 13, 'n': 14, 'o': 15, 'p': 16, 'q': 17, 'r': 18, 's': 19, 't': 20, 'u': 21, 'v': 22, 'w': 23, 'x': 24, 'y': 25, 'z': 26} \n",
      "\n",
      "{0: '\\n', 1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z'}\n"
     ]
    }
   ],
   "source": [
    "#构建字典\n",
    "char_to_ix = {ch:i for i, ch in enumerate(sorted(chars))}\n",
    "ix_to_char= {i:ch for i, ch in enumerate(sorted(chars))}\n",
    "\n",
    "print(char_to_ix,\"\\n\")\n",
    "print(ix_to_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "roman-moses",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip(gradients, maxValue):\n",
    "    \"\"\"\n",
    "    使用maxValue来修剪梯度\n",
    "    \n",
    "    参数：\n",
    "        gradients -- 字典类型，包含了以下参数：\"dWaa\", \"dWax\", \"dWya\", \"db\", \"dby\"\n",
    "        maxValue -- 阈值，把梯度值限制在[-maxValue, maxValue]内\n",
    "        \n",
    "    返回：\n",
    "        gradients -- 修剪后的梯度\n",
    "    \"\"\"\n",
    "    #获取参数\n",
    "    dWaa, dWax, dWya, db, dby = gradients[\"dWaa\"], gradients[\"dWax\"], gradients[\"dWya\"], gradients[\"db\"], gradients[\"dby\"]\n",
    "    \n",
    "    #梯度修剪\n",
    "    for gradient in [dWaa, dWax, dWya, db, dby]:\n",
    "        np.clip(gradient, -maxValue, maxValue, out=gradient)\n",
    "    \n",
    "    gradients = {\"dWaa\": dWaa, \"dWax\": dWax, \"dWya\": dWya, \"db\": db, \"dby\": dby}\n",
    "    \n",
    "    return gradients\n",
    "\n",
    "#test\n",
    "# np.random.seed(3)\n",
    "# dWax = np.random.randn(5,3)*10\n",
    "# dWaa = np.random.randn(5,5)*10\n",
    "# dWya = np.random.randn(2,5)*10\n",
    "# db = np.random.randn(5,1)*10\n",
    "# dby = np.random.randn(2,1)*10\n",
    "# gradients = {\"dWax\": dWax, \"dWaa\": dWaa, \"dWya\": dWya, \"db\": db, \"dby\": dby}\n",
    "# gradients = clip(gradients, 10)\n",
    "# print(\"gradients[\\\"dWaa\\\"][1][2] =\", gradients[\"dWaa\"][1][2])\n",
    "# print(\"gradients[\\\"dWax\\\"][3][1] =\", gradients[\"dWax\"][3][1])\n",
    "# print(\"gradients[\\\"dWya\\\"][1][2] =\", gradients[\"dWya\"][1][2])\n",
    "# print(\"gradients[\\\"db\\\"][4] =\", gradients[\"db\"][4])\n",
    "# print(\"gradients[\\\"dby\\\"][1] =\", gradients[\"dby\"][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "painful-journalist",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(parameters, char_to_ix, seed):\n",
    "    \"\"\"\n",
    "    根据RNN输出的概率分布序列对字符序列进行采样\n",
    "    \n",
    "    参数：\n",
    "        parameters -- 包含了Waa, Wax, Wya, by, b的字典\n",
    "        char_to_ix -- 字符映射到索引的字典\n",
    "        seed -- 随机种子\n",
    "        \n",
    "    返回：\n",
    "        indices -- 包含采样字符索引的长度为n的列表。\n",
    "    \"\"\"\n",
    "    \n",
    "    #从parameters中获取参数\n",
    "    Waa, Wax, Wya, by, b = parameters[\"Waa\"], parameters[\"Wax\"], parameters[\"Wya\"], parameters[\"by\"], parameters[\"b\"]\n",
    "    vocab_size = by.shape[0]\n",
    "    n_a = Waa.shape[1]\n",
    "    \n",
    "    #1st step:创建one-hot vector\n",
    "    x = np.zeros((vocab_size,1))\n",
    "    \n",
    "    #初始化a_prev\n",
    "    a_prev = np.zeros((n_a,1))\n",
    "    \n",
    "    #创建索引的空列表，用来储存生成的字符的索引\n",
    "    indices = []\n",
    "    \n",
    "    #用IDX来检测是否遇到换行符\n",
    "    idx = -1\n",
    "    \n",
    "    #循环时间步t，每个时间步选择一个字符，讲字符添加到\"indices\"上，如果我们达到50字符就结束\n",
    "    counter = 0\n",
    "    newline_character = char_to_ix[\"\\n\"]\n",
    "    \n",
    "    while (idx != newline_character and counter < 50):\n",
    "        #2nd step: forward propagation\n",
    "        a = np.tanh(np.dot(Waa, a_prev) + np.dot(Wax, x) + b)\n",
    "        z = np.dot(Wya,a) + by\n",
    "        y = cllm_utils.softmax(z)\n",
    "        \n",
    "        #设定随机种子\n",
    "        np.random.seed(counter + seed)\n",
    "        \n",
    "        #3rd step: 从y中随机选取字符，并储存索引,p为对应索引取到的概率（算出来的y）\n",
    "        idx = np.random.choice(vocab_size, p=y.ravel())\n",
    "        indices.append(idx)\n",
    "        \n",
    "        #4th step: 将输入的字符重写为与采样索引对应的字符,namely对应的one-hot向量\n",
    "        x = np.zeros((vocab_size,1))\n",
    "        x[idx] = 1\n",
    "        \n",
    "        #更新a_prev为a\n",
    "        a_prev = a\n",
    "        \n",
    "        #累加器\n",
    "        seed +=1\n",
    "        counter +=1\n",
    "    \n",
    "    if(counter == 50):\n",
    "        indices.append(newline_character)\n",
    "        \n",
    "    return indices\n",
    "\n",
    "#test\n",
    "# np.random.seed(2)\n",
    "# _, n_a = 20, 100\n",
    "# Wax, Waa, Wya = np.random.randn(n_a, vocab_size), np.random.randn(n_a, n_a), np.random.randn(vocab_size, n_a)\n",
    "# b, by = np.random.randn(n_a, 1), np.random.randn(vocab_size, 1)\n",
    "# parameters = {\"Wax\": Wax, \"Waa\": Waa, \"Wya\": Wya, \"b\": b, \"by\": by}\n",
    "\n",
    "\n",
    "# indices = sample(parameters, char_to_ix, 0)\n",
    "# print(\"Sampling:\")\n",
    "# print(\"list of sampled indices:\", indices)\n",
    "# print(\"list of sampled characters:\", [ix_to_char[i] for i in indices])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "hairy-twist",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize(X, Y, a_prev, parameters, learning_rate = 0.01):\n",
    "    \"\"\"\n",
    "    执行训练模型的单步优化。\n",
    "    \n",
    "    参数：\n",
    "        X -- 整数列表，其中每个整数映射到词汇表中的字符。\n",
    "        Y -- 整数列表，与X完全相同，但向左移动了一个索引。\n",
    "        a_prev -- 上一个隐藏状态\n",
    "        parameters -- 字典，包含了以下参数：\n",
    "                        Wax -- 权重矩阵乘以输入，维度为(n_a, n_x)\n",
    "                        Waa -- 权重矩阵乘以隐藏状态，维度为(n_a, n_a)\n",
    "                        Wya -- 隐藏状态与输出相关的权重矩阵，维度为(n_y, n_a)\n",
    "                        b -- 偏置，维度为(n_a, 1)\n",
    "                        by -- 隐藏状态与输出相关的权重偏置，维度为(n_y, 1)\n",
    "        learning_rate -- 模型学习的速率\n",
    "    \n",
    "    返回：\n",
    "        loss -- 损失函数的值（交叉熵损失）\n",
    "        gradients -- 字典，包含了以下参数：\n",
    "                        dWax -- 输入到隐藏的权值的梯度，维度为(n_a, n_x)\n",
    "                        dWaa -- 隐藏到隐藏的权值的梯度，维度为(n_a, n_a)\n",
    "                        dWya -- 隐藏到输出的权值的梯度，维度为(n_y, n_a)\n",
    "                        db -- 偏置的梯度，维度为(n_a, 1)\n",
    "                        dby -- 输出偏置向量的梯度，维度为(n_y, 1)\n",
    "        a[len(X)-1] -- 最后的隐藏状态，维度为(n_a, 1)\n",
    "    \"\"\"\n",
    "    #前向传播\n",
    "    loss, cache = cllm_utils.rnn_forward(X, Y, a_prev, parameters)\n",
    "    \n",
    "    #反向传播\n",
    "    gradients, a = cllm_utils.rnn_backward(X, Y, parameters, cache)\n",
    "    \n",
    "    #梯度修剪\n",
    "    gradients = clip(gradients,5)\n",
    "    \n",
    "    #更新参数\n",
    "    parameters = cllm_utils.update_parameters(parameters,gradients,learning_rate)\n",
    "    \n",
    "    return loss, gradients, a[len(X)-1]\n",
    "\n",
    "#test\n",
    "# np.random.seed(1)\n",
    "# vocab_size, n_a = 27, 100\n",
    "# a_prev = np.random.randn(n_a, 1)\n",
    "# Wax, Waa, Wya = np.random.randn(n_a, vocab_size), np.random.randn(n_a, n_a), np.random.randn(vocab_size, n_a)\n",
    "# b, by = np.random.randn(n_a, 1), np.random.randn(vocab_size, 1)\n",
    "# parameters = {\"Wax\": Wax, \"Waa\": Waa, \"Wya\": Wya, \"b\": b, \"by\": by}\n",
    "# X = [12,3,5,11,22,3]\n",
    "# Y = [4,14,11,22,25, 26]\n",
    "\n",
    "# loss, gradients, a_last = optimize(X, Y, a_prev, parameters, learning_rate = 0.01)\n",
    "# print(\"Loss =\", loss)\n",
    "# print(\"gradients[\\\"dWaa\\\"][1][2] =\", gradients[\"dWaa\"][1][2])\n",
    "# print(\"np.argmax(gradients[\\\"dWax\\\"]) =\", np.argmax(gradients[\"dWax\"]))\n",
    "# print(\"gradients[\\\"dWya\\\"][1][2] =\", gradients[\"dWya\"][1][2])\n",
    "# print(\"gradients[\\\"db\\\"][4] =\", gradients[\"db\"][4])\n",
    "# print(\"gradients[\\\"dby\\\"][1] =\", gradients[\"dby\"][1])\n",
    "# print(\"a_last[4] =\", a_last[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "unable-schema",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(data, ix_to_char, char_to_ix, num_iterations=3500, \n",
    "          n_a=50, dino_names=7,vocab_size=27):\n",
    "    \"\"\"\n",
    "    训练模型并生成恐龙名字\n",
    "    \n",
    "    参数：\n",
    "        data -- 语料库\n",
    "        ix_to_char -- 索引映射字符字典\n",
    "        char_to_ix -- 字符映射索引字典\n",
    "        num_iterations -- 迭代次数\n",
    "        n_a -- RNN单元数量\n",
    "        dino_names -- 每次迭代中采样的数量\n",
    "        vocab_size -- 在文本中的唯一字符的数量\n",
    "    \n",
    "    返回：\n",
    "        parameters -- 学习后了的参数\n",
    "    \"\"\"\n",
    "    #从vocab_size中获取n_x, n_y\n",
    "    n_x, n_y = vocab_size, vocab_size\n",
    "    \n",
    "    #初始化参数,输入和输出的vector长度应该一样\n",
    "    parameters = cllm_utils.initialize_parameters(n_a, n_x, n_y)\n",
    "    \n",
    "    #初始化损失\n",
    "    loss = cllm_utils.get_initial_loss(vocab_size, dino_names)\n",
    "    \n",
    "    #构建全部的恐龙名称\n",
    "    with open(\"dinos.txt\") as f:\n",
    "        examples = f.readlines()\n",
    "    #小写恐龙的名字，并且去除换行符\"\\n\"\n",
    "    examples = [x.lower().strip() for x in examples]\n",
    "    \n",
    "    #打乱全部恐龙的名称\n",
    "    np.random.seed(0)\n",
    "    np.random.shuffle(examples)\n",
    "    \n",
    "    #初始化LSTM的隐藏状态\n",
    "    a_prev = np.zeros((n_a,1))\n",
    "    \n",
    "    #循环训练\n",
    "    for j in range(num_iterations):\n",
    "        #定义一个训练样本\n",
    "        index = j % len(examples)   #防止循环数大于example的个数\n",
    "        X = [None] + [char_to_ix[ch] for ch in examples[index]]  #构建出每个字符对应的one-hot编码，第一个None对应0向量\n",
    "        Y = X[1:]  + [char_to_ix[\"\\n\"]]  #Y(t)对应X(t+1), 加上每个字符串结尾的\\n转行符\n",
    "        \n",
    "        #执行单步优化：前向传播->反向传播->梯度修剪->更新参数\n",
    "        #选择学习率为0.01\n",
    "        curr_loss, gradients, a_prev = optimize(X, Y, a_prev, parameters)\n",
    "        \n",
    "        #使用延迟来保持损失平滑，加速训练\n",
    "        loss = cllm_utils.smooth(loss, curr_loss)\n",
    "        \n",
    "        #每2000代，通过sample()生成“\\n”字符,检查模型是否学习正确\n",
    "        if j % 2000== 0:\n",
    "            print(\"第\" + str(j+1) + \"次迭代，损失值为:\" + str(loss))\n",
    "            \n",
    "            seed = 0\n",
    "            for name in range(dino_names):\n",
    "                #采样\n",
    "                sampled_indices = sample(parameters, char_to_ix, seed)\n",
    "                cllm_utils.print_sample(sampled_indices, ix_to_char)\n",
    "                \n",
    "                #为了得到相同的效果，种子+1\n",
    "                seed += 1\n",
    "                \n",
    "            print(\"\\n\")\n",
    "            \n",
    "    return parameters\n",
    "\n",
    "#test\n",
    "# #开始时间\n",
    "# start_time = time.clock()\n",
    "\n",
    "# #开始训练\n",
    "# parameters = model(data, ix_to_char, char_to_ix, num_iterations=3500)\n",
    "\n",
    "# #结束时间\n",
    "# end_time = time.clock()\n",
    "\n",
    "# #计算时差\n",
    "# minium = end_time - start_time\n",
    "\n",
    "# print(\"执行了：\" + str(int(minium / 60)) + \"分\" + str(int(minium%60)) + \"秒\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "needed-foster",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ersac\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:2: DeprecationWarning: time.clock has been deprecated in Python 3.3 and will be removed from Python 3.8: use time.perf_counter or time.process_time instead\n",
      "  \n",
      "Using TensorFlow backend.\n",
      "C:\\Users\\ersac\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\ersac\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\ersac\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\ersac\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\ersac\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\ersac\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\Users\\ersac\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\ersac\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\ersac\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\ersac\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\ersac\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\ersac\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading text data...\n",
      "Creating training set...\n",
      "number of training examples: 31412\n",
      "Vectorizing training set...\n",
      "Loading model...\n",
      "WARNING:tensorflow:From C:\\Users\\ersac\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From C:\\Users\\ersac\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "执行了：0分16秒\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ersac\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\saving.py:384: UserWarning: Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n",
      "  warnings.warn('Error in loading the saved optimizer '\n",
      "C:\\Users\\ersac\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:15: DeprecationWarning: time.clock has been deprecated in Python 3.3 and will be removed from Python 3.8: use time.perf_counter or time.process_time instead\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    }
   ],
   "source": [
    "#开始时间\n",
    "start_time = time.clock()\n",
    "\n",
    "from keras.callbacks import LambdaCallback\n",
    "from keras.models import Model, load_model, Sequential\n",
    "from keras.layers import Dense, Activation, Dropout, Input, Masking\n",
    "from keras.layers import LSTM\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from shakespeare_utils import *\n",
    "import sys\n",
    "import io\n",
    "\n",
    "#结束时间\n",
    "end_time = time.clock()\n",
    "\n",
    "#计算时差\n",
    "minium = end_time - start_time\n",
    "\n",
    "print(\"执行了：\" + str(int(minium / 60)) + \"分\" + str(int(minium%60)) + \"秒\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
