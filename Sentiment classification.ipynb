{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "mineral-puzzle",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import emo_utils\n",
    "import emoji\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "fewer-mercury",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Miss you so much ❤️\n"
     ]
    }
   ],
   "source": [
    "X_train, Y_train = emo_utils.read_csv('data/train_emoji.csv')\n",
    "X_test, Y_test = emo_utils.read_csv('data/test.csv')\n",
    "\n",
    "maxLen = len(max(X_train, key=len).split())\n",
    "index  = 3\n",
    "print(X_train[index], emo_utils.label_to_emoji(Y_train[index]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "level-sheet",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3对应的独热编码是[0. 0. 0. 1. 0.]\n"
     ]
    }
   ],
   "source": [
    "Y_oh_train = emo_utils.convert_to_one_hot(Y_train, C=5)\n",
    "Y_oh_test  = emo_utils.convert_to_one_hot(Y_test,  C=5)\n",
    "#test\n",
    "# index = 0 \n",
    "# print(\"{0}对应的独热编码是{1}\".format(Y_train[index], Y_oh_train[index]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "permanent-present",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "单词cucumber对应的索引是：113317\n",
      "索引113317对应的单词是：cucumber\n"
     ]
    }
   ],
   "source": [
    "word_to_index, index_to_word, word_to_vec_map = emo_utils.read_glove_vecs('data/glove.6B.50d.txt')\n",
    "#word_to_index：字典类型的词汇（400,001个）与索引的映射（有效范围：0-400,000）\n",
    "#index_to_word：字典类型的索引与词汇之间的映射。\n",
    "#word_to_vec_map：字典类型的词汇与对应GloVe向量的映射。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "raising-uruguay",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1st step: transform and average\n",
    "def sentence_to_avg(sentence, word_to_vec_map):\n",
    "    \"\"\"\n",
    "    将句子转换为单词列表，提取其GloVe向量，然后将其平均。\n",
    "    \n",
    "    参数：\n",
    "        sentence -- 字符串类型，从X中获取的样本。\n",
    "        word_to_vec_map -- 字典类型，单词映射到50维的向量的字典\n",
    "        \n",
    "    返回：\n",
    "        avg -- 对句子的均值编码，维度为(50,)\n",
    "    \"\"\"\n",
    "    #转化为小写,并分割句子\n",
    "    words = sentence.lower().split()\n",
    "    \n",
    "    #初始化average\n",
    "    avg = np.zeros(50,)\n",
    "    \n",
    "    #对sentence中的所有单词求平均\n",
    "    for w in words:\n",
    "        avg += word_to_vec_map[w]\n",
    "    avg = np.divide(avg, len(words))\n",
    "    \n",
    "    return avg\n",
    "\n",
    "#test\n",
    "# avg = sentence_to_avg(\"Morrocan couscous is my favorite dish\", word_to_vec_map)\n",
    "# print(\"avg = \", avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "engaging-potential",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第0轮, 损失为1.9520498812810072\n",
      "Accuracy: 0.3484848484848485\n",
      "第100轮, 损失为0.07971818726014807\n",
      "Accuracy: 0.9318181818181818\n",
      "第200轮, 损失为0.04456369243681402\n",
      "Accuracy: 0.9545454545454546\n",
      "第300轮, 损失为0.03432267378786059\n",
      "Accuracy: 0.9696969696969697\n"
     ]
    }
   ],
   "source": [
    "#2nd step: train and optimize\n",
    "def model(X, Y, word_to_vec_map, learning_rate=0.01, num_iterations=400):\n",
    "    \"\"\"\n",
    "    在numpy中训练词向量模型。\n",
    "    \n",
    "    参数：\n",
    "        X -- 输入的字符串类型的数据，维度为(m, 1)。\n",
    "        Y -- 对应的标签，0-7的数组，维度为(m, 1)。\n",
    "        word_to_vec_map -- 字典类型的单词到50维词向量的映射。\n",
    "        learning_rate -- 学习率.\n",
    "        num_iterations -- 迭代次数。\n",
    "        \n",
    "    返回：\n",
    "        pred -- 预测的向量，维度为(m, 1)。\n",
    "        W -- 权重参数，维度为(n_y, n_h)。\n",
    "        b -- 偏置参数，维度为(n_y,)\n",
    "    \"\"\"\n",
    "    np.random.seed(1)\n",
    "    \n",
    "    #定义训练数量\n",
    "    m = Y.shape[0]\n",
    "    n_y = 5\n",
    "    n_h = 50\n",
    "    \n",
    "    #使用Xavier初始化参数\n",
    "    W = np.random.randn(n_y, n_h) / np.sqrt(n_h)\n",
    "    b = np.zeros((n_y,))\n",
    "    \n",
    "    #将Y转化成独热编码\n",
    "    Y_oh = emo_utils.convert_to_one_hot(Y, C=n_y)\n",
    "    \n",
    "    #优化循环\n",
    "    for t in range(num_iterations):\n",
    "        for i in range(m):\n",
    "            #对第i个样本运行Model\n",
    "            avg = sentence_to_avg(X[i],word_to_vec_map)\n",
    "        \n",
    "            #forward propagation\n",
    "            z = np.dot(W, avg) + b\n",
    "            a = emo_utils.softmax(z)\n",
    "        \n",
    "            #计算损失\n",
    "            cost = -np.sum(Y_oh[i]*np.log(a))\n",
    "        \n",
    "            #backward propagation\n",
    "            dz = a - Y_oh[i]\n",
    "            dW = np.dot(dz.reshape(n_y,1),avg.reshape(1,n_h)) #初始化的dz, avg是rank=1的,要转化为矩阵\n",
    "            db = dz\n",
    "        \n",
    "            #optimize\n",
    "            W = W - learning_rate * dW\n",
    "            b = b - learning_rate * db\n",
    "            \n",
    "        if t % 100 == 0:\n",
    "            print(\"第{t}轮, 损失为{cost}\".format(t=t,cost=cost))\n",
    "            pred = emo_utils.predict(X, Y, W, b, word_to_vec_map)\n",
    "    \n",
    "    return pred, W, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "funky-procurement",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第0轮, 损失为1.9520498812810072\n",
      "Accuracy: 0.3484848484848485\n",
      "第100轮, 损失为0.07971818726014807\n",
      "Accuracy: 0.9318181818181818\n",
      "第200轮, 损失为0.04456369243681402\n",
      "Accuracy: 0.9545454545454546\n",
      "第300轮, 损失为0.03432267378786059\n",
      "Accuracy: 0.9696969696969697\n",
      "=====训练集====\n",
      "Accuracy: 0.9772727272727273\n",
      "=====测试集====\n",
      "Accuracy: 0.8571428571428571\n"
     ]
    }
   ],
   "source": [
    "#test\n",
    "pred, W, b = model(X_train, Y_train, word_to_vec_map)\n",
    "print(\"=====训练集====\")\n",
    "pred_train = emo_utils.predict(X_train, Y_train, W, b, word_to_vec_map)\n",
    "print(\"=====测试集====\")\n",
    "pred_test = emo_utils.predict(X_test, Y_test, W, b, word_to_vec_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "unsigned-remainder",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8333333333333334\n",
      "\n",
      "i adore you ❤️\n",
      "i love you ❤️\n",
      "funny lol 😄\n",
      "lets play with a ball ⚾\n",
      "food is ready 🍴\n",
      "you are not happy ❤️\n"
     ]
    }
   ],
   "source": [
    "#test on my_senteces\n",
    "X_my_sentences = np.array([\"i adore you\", \"i love you\", \"funny lol\", \"lets play with a ball\", \"food is ready\", \"you are not happy\"])\n",
    "Y_my_labels = np.array([[0], [0], [2], [1], [4],[3]])\n",
    "\n",
    "pred = emo_utils.predict(X_my_sentences, Y_my_labels , W, b, word_to_vec_map)\n",
    "emo_utils.print_predictions(X_my_sentences, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "noble-applicant",
   "metadata": {},
   "source": [
    "即使你只有128个训练样本，你也可以得到很好地表情符号模型，因为词向量是训练好了的，它会给你一个较好的概括能力。\n",
    "\n",
    "Emojifier-V1是有缺陷的，比如它不会把“This movie is not good and not enjoyable”划分为不好一类，因为它只是将所有单词的向量做了平均，没有关心过顺序。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "after-combine",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "C:\\Users\\ersac\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\ersac\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\ersac\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\ersac\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\ersac\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\ersac\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\Users\\ersac\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\ersac\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\ersac\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\ersac\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\ersac\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\ersac\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "import keras\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Input, Dropout, LSTM, Activation\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "np.random.seed(1)\n",
    "from keras.initializers import glorot_uniform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "million-sunglasses",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentences_to_indices(X, word_to_index, max_len):\n",
    "    \"\"\"\n",
    "    输入的是X（字符串类型的句子的数组），再转化为对应的句子列表，\n",
    "    输出的是能够让Embedding()函数接受的列表或矩阵（参见图4）。\n",
    "    \n",
    "    参数：\n",
    "        X -- 句子数组，维度为(m, 1)\n",
    "        word_to_index -- 字典类型的单词到索引的映射\n",
    "        max_len -- 最大句子的长度，数据集中所有的句子的长度都不会超过它。\n",
    "        \n",
    "    返回：\n",
    "        X_indices -- 对应于X中的单词索引数组，维度为(m, max_len)\n",
    "    \"\"\"\n",
    "    m = X.shape[0]\n",
    "    X_indices = np.zeros((m, max_len))\n",
    "    \n",
    "    for i in range(m):\n",
    "        j = 0\n",
    "        sentence_words = X[i].lower().split()\n",
    "        for w in sentence_words:\n",
    "            X_indices[i][j] = word_to_index[w]\n",
    "            j += 1\n",
    "    \n",
    "    return X_indices\n",
    "\n",
    "#test\n",
    "# X1 = np.array([\"funny lol\", \"lets play baseball\", \"food is ready for you\"])\n",
    "# X1_indices = sentences_to_indices(X1,word_to_index, max_len = 5)\n",
    "# print(\"X1 =\", X1)\n",
    "# print(\"X1_indices =\", X1_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "convinced-smooth",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights[0][1][3] = -0.3403\n"
     ]
    }
   ],
   "source": [
    "def pretrained_embedding_layer(word_to_vec_map, word_to_index):\n",
    "    \"\"\"\n",
    "    创建Keras Embedding()层，加载已经训练好了的50维GloVe向量\n",
    "    \n",
    "    参数：\n",
    "        word_to_vec_map -- 字典类型的单词与词嵌入的映射\n",
    "        word_to_index -- 字典类型的单词到词汇表（400,001个单词）的索引的映射。\n",
    "        \n",
    "    返回：\n",
    "        embedding_layer() -- 训练好了的Keras的实体层。\n",
    "    \"\"\"\n",
    "    vocab_len = len(word_to_index) + 1\n",
    "    emb_dim = word_to_vec_map[\"cucumber\"].shape[0]\n",
    "    \n",
    "    #初始化嵌入矩阵\n",
    "    emb_matrix = np.zeros((vocab_len, emb_dim))\n",
    "    \n",
    "    #讲嵌入矩阵的每行的“index”设置为词汇\"index\"的词向量表示\n",
    "    for word, index in word_to_index.items():\n",
    "        emb_matrix[index,:] = word_to_vec_map[word]\n",
    "        \n",
    "    #定义Keras的embedding层\n",
    "    embedding_layer = Embedding(vocab_len, emb_dim, trainable=False)\n",
    "    \n",
    "    #构建embedding层\n",
    "    embedding_layer.build((None,))\n",
    "    \n",
    "    #讲嵌入层的权重设置为嵌入矩阵\n",
    "    embedding_layer.set_weights([emb_matrix])\n",
    "    \n",
    "    return embedding_layer\n",
    "\n",
    "#test\n",
    "# embedding_layer = pretrained_embedding_layer(word_to_vec_map, word_to_index)\n",
    "# print(\"weights[0][1][3] =\", embedding_layer.get_weights()[0][1][3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "artistic-strap",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Emojify_V2(input_shape, word_to_vec_map, word_to_index):\n",
    "    \"\"\"\n",
    "    实现Emojify-V2模型的计算图\n",
    "    \n",
    "    参数：\n",
    "        input_shape -- 输入的维度，通常是(max_len,)\n",
    "        word_to_vec_map -- 字典类型的单词与词嵌入的映射。\n",
    "        word_to_index -- 字典类型的单词到词汇表（400,001个单词）的索引的映射。\n",
    "    \n",
    "    返回：\n",
    "        model -- Keras模型实体\n",
    "    \"\"\"\n",
    "    #定义sentece_indices为计算图的输入，维度为(input_shape,),类型为dtype\"int32\"\n",
    "    sentence_indices = Input(input_shape, dtype=\"int32\")\n",
    "    \n",
    "    #创建embedding层\n",
    "    embedding_layer = pretrained_embedding_layer(word_to_vec_map, word_to_index)\n",
    "    \n",
    "    #通过嵌入层传播sentence_indices，得到嵌入的结果\n",
    "    embeddings = embedding_layer(sentence_indices)\n",
    "    \n",
    "    #通过带有128维隐藏状态的LSTM层传播嵌入\n",
    "    #需要注意的是，这里是图形的第一层，返回的输出应该是一批序列\n",
    "    X = LSTM(128, return_sequences=True)(embeddings)\n",
    "    #使用dropout\n",
    "    X = Dropout(0.5)(X)\n",
    "    #通过第二层128维隐藏状态的LSTM层传播X\n",
    "    #这时是many-to-one模型，输出是最后一个y\n",
    "    X = LSTM(128, return_sequences=False)(X)\n",
    "    #使用dropout\n",
    "    X = Dropout(0.5)(X)\n",
    "    #Dense层全连接\n",
    "    X = Dense(5)(X)\n",
    "    #添加softmax激活\n",
    "    X = Activation(\"softmax\")(X)\n",
    "    \n",
    "    #创建模型实体\n",
    "    model = Model(inputs=sentence_indices, outputs=X)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "moved-title",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "embedding_2 (Embedding)      (None, 10, 50)            20000050  \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 10, 128)           91648     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 10, 128)           0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 128)               131584    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 5)                 645       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 5)                 0         \n",
      "=================================================================\n",
      "Total params: 20,223,927\n",
      "Trainable params: 223,877\n",
      "Non-trainable params: 20,000,050\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Emojify_V2((10,), word_to_vec_map, word_to_index)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "female-character",
   "metadata": {},
   "outputs": [],
   "source": [
    "#编译模型\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "threaded-korea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\ersac\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Epoch 1/50\n",
      "132/132 [==============================] - 2s 18ms/step - loss: 1.6100 - accuracy: 0.1970\n",
      "Epoch 2/50\n",
      "132/132 [==============================] - 0s 2ms/step - loss: 1.5254 - accuracy: 0.3030\n",
      "Epoch 3/50\n",
      "132/132 [==============================] - 0s 2ms/step - loss: 1.4885 - accuracy: 0.3182\n",
      "Epoch 4/50\n",
      "132/132 [==============================] - 0s 2ms/step - loss: 1.4148 - accuracy: 0.3939\n",
      "Epoch 5/50\n",
      "132/132 [==============================] - 0s 2ms/step - loss: 1.3107 - accuracy: 0.4773\n",
      "Epoch 6/50\n",
      "132/132 [==============================] - 0s 2ms/step - loss: 1.1912 - accuracy: 0.5455\n",
      "Epoch 7/50\n",
      "132/132 [==============================] - 0s 3ms/step - loss: 1.1405 - accuracy: 0.5000\n",
      "Epoch 8/50\n",
      "132/132 [==============================] - 0s 2ms/step - loss: 0.9976 - accuracy: 0.6288\n",
      "Epoch 9/50\n",
      "132/132 [==============================] - 0s 2ms/step - loss: 0.8248 - accuracy: 0.7424\n",
      "Epoch 10/50\n",
      "132/132 [==============================] - 0s 2ms/step - loss: 0.7721 - accuracy: 0.6970\n",
      "Epoch 11/50\n",
      "132/132 [==============================] - 0s 2ms/step - loss: 0.6461 - accuracy: 0.7727\n",
      "Epoch 12/50\n",
      "132/132 [==============================] - 0s 2ms/step - loss: 0.5790 - accuracy: 0.8030\n",
      "Epoch 13/50\n",
      "132/132 [==============================] - 0s 2ms/step - loss: 0.4678 - accuracy: 0.8485\n",
      "Epoch 14/50\n",
      "132/132 [==============================] - 0s 2ms/step - loss: 0.4914 - accuracy: 0.8106\n",
      "Epoch 15/50\n",
      "132/132 [==============================] - 0s 2ms/step - loss: 0.4647 - accuracy: 0.8258\n",
      "Epoch 16/50\n",
      "132/132 [==============================] - 0s 2ms/step - loss: 0.3228 - accuracy: 0.8712\n",
      "Epoch 17/50\n",
      "132/132 [==============================] - 0s 2ms/step - loss: 0.3557 - accuracy: 0.8864\n",
      "Epoch 18/50\n",
      "132/132 [==============================] - 0s 2ms/step - loss: 0.5724 - accuracy: 0.8485\n",
      "Epoch 19/50\n",
      "132/132 [==============================] - 0s 2ms/step - loss: 0.4483 - accuracy: 0.8333\n",
      "Epoch 20/50\n",
      "132/132 [==============================] - 0s 2ms/step - loss: 0.3570 - accuracy: 0.8788\n",
      "Epoch 21/50\n",
      "132/132 [==============================] - 0s 2ms/step - loss: 0.3733 - accuracy: 0.8258\n",
      "Epoch 22/50\n",
      "132/132 [==============================] - 0s 2ms/step - loss: 0.3615 - accuracy: 0.8712\n",
      "Epoch 23/50\n",
      "132/132 [==============================] - 0s 2ms/step - loss: 0.2990 - accuracy: 0.8712\n",
      "Epoch 24/50\n",
      "132/132 [==============================] - 0s 2ms/step - loss: 0.3387 - accuracy: 0.8864\n",
      "Epoch 25/50\n",
      "132/132 [==============================] - 0s 2ms/step - loss: 0.2430 - accuracy: 0.9394\n",
      "Epoch 26/50\n",
      "132/132 [==============================] - 0s 3ms/step - loss: 0.1867 - accuracy: 0.9470\n",
      "Epoch 27/50\n",
      "132/132 [==============================] - 0s 2ms/step - loss: 0.1421 - accuracy: 0.9697\n",
      "Epoch 28/50\n",
      "132/132 [==============================] - 0s 2ms/step - loss: 0.1502 - accuracy: 0.9621\n",
      "Epoch 29/50\n",
      "132/132 [==============================] - 0s 2ms/step - loss: 0.1043 - accuracy: 0.9773\n",
      "Epoch 30/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.1094 - accuracy: 0.9621\n",
      "Epoch 31/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.1593 - accuracy: 0.9470\n",
      "Epoch 32/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.1200 - accuracy: 0.9621\n",
      "Epoch 33/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.0808 - accuracy: 0.9848\n",
      "Epoch 34/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.1158 - accuracy: 0.9773\n",
      "Epoch 35/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.2279 - accuracy: 0.9167\n",
      "Epoch 36/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.0828 - accuracy: 0.9773\n",
      "Epoch 37/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.2213 - accuracy: 0.9242\n",
      "Epoch 38/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.1083 - accuracy: 0.9470\n",
      "Epoch 39/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.1416 - accuracy: 0.9470\n",
      "Epoch 40/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.1460 - accuracy: 0.9545\n",
      "Epoch 41/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.0966 - accuracy: 0.9621\n",
      "Epoch 42/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.1102 - accuracy: 0.9697\n",
      "Epoch 43/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.0682 - accuracy: 0.9773\n",
      "Epoch 44/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.0681 - accuracy: 0.9848\n",
      "Epoch 45/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.3102 - accuracy: 0.8939\n",
      "Epoch 46/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.3442 - accuracy: 0.8712\n",
      "Epoch 47/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.3552 - accuracy: 0.8864\n",
      "Epoch 48/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.1654 - accuracy: 0.9545\n",
      "Epoch 49/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.2186 - accuracy: 0.9167\n",
      "Epoch 50/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.0945 - accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x2c1127358d0>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#训练集上的结果\n",
    "X_train_indices = sentences_to_indices(X_train, word_to_index, 10)\n",
    "Y_train_oh = emo_utils.convert_to_one_hot(Y_train, C = 5)\n",
    "model.fit(X_train_indices, Y_train_oh, epochs = 50, batch_size = 32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "paperback-fitness",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56/56 [==============================] - 0s 7ms/step\n",
      "Test accuracy =  0.8392857313156128\n"
     ]
    }
   ],
   "source": [
    "#测试集上的结果\n",
    "X_test_indices = sentences_to_indices(X_test, word_to_index, max_len = 10)\n",
    "Y_test_oh = emo_utils.convert_to_one_hot(Y_test, C = 5)\n",
    "loss, acc = model.evaluate(X_test_indices, Y_test_oh)\n",
    "\n",
    "print(\"Test accuracy = \", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "sweet-example",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正确表情：😄   预测结果： he got a very nice raise\t❤️\n",
      "正确表情：😄   预测结果： she got me a nice present\t❤️\n",
      "正确表情：😄   预测结果： he is a good friend\t❤️\n",
      "正确表情：😞   预测结果： work is hard\t😄\n",
      "正确表情：😞   预测结果： This girl is messing with me\t❤️\n",
      "正确表情：❤️   预测结果： I love taking breaks\t😞\n",
      "正确表情：😄   预测结果： you brighten my day\t❤️\n",
      "正确表情：😄   预测结果： will you be my valentine\t❤️\n",
      "正确表情：😄   预测结果： I like to laugh\t❤️\n"
     ]
    }
   ],
   "source": [
    "#查看错误例子\n",
    "C = 5\n",
    "y_test_oh = np.eye(C)[Y_test.reshape(-1)]\n",
    "X_test_indices = sentences_to_indices(X_test, word_to_index, 10)\n",
    "pred = model.predict(X_test_indices)\n",
    "for i in range(len(X_test)):\n",
    "    x = X_test_indices\n",
    "    num = np.argmax(pred[i])\n",
    "    if(num != Y_test[i]):\n",
    "        print('正确表情：'+ emo_utils.label_to_emoji(Y_test[i]) + '   预测结果： '+ X_test[i] + emo_utils.label_to_emoji(num).strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "express-timer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "you are not happy 😞\n"
     ]
    }
   ],
   "source": [
    "#测试my sentences\n",
    "x_test = np.array(['you are not happy'])\n",
    "X_test_indices = sentences_to_indices(x_test, word_to_index, maxLen)\n",
    "print(x_test[0] +' '+  emo_utils.label_to_emoji(np.argmax(model.predict(X_test_indices))))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
