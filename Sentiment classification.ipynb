{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "mineral-puzzle",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import emo_utils\n",
    "import emoji\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "fewer-mercury",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Miss you so much â¤ï¸\n"
     ]
    }
   ],
   "source": [
    "X_train, Y_train = emo_utils.read_csv('data/train_emoji.csv')\n",
    "X_test, Y_test = emo_utils.read_csv('data/test.csv')\n",
    "\n",
    "maxLen = len(max(X_train, key=len).split())\n",
    "index  = 3\n",
    "print(X_train[index], emo_utils.label_to_emoji(Y_train[index]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "level-sheet",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3å¯¹åº”çš„ç‹¬çƒ­ç¼–ç æ˜¯[0. 0. 0. 1. 0.]\n"
     ]
    }
   ],
   "source": [
    "Y_oh_train = emo_utils.convert_to_one_hot(Y_train, C=5)\n",
    "Y_oh_test  = emo_utils.convert_to_one_hot(Y_test,  C=5)\n",
    "#test\n",
    "# index = 0 \n",
    "# print(\"{0}å¯¹åº”çš„ç‹¬çƒ­ç¼–ç æ˜¯{1}\".format(Y_train[index], Y_oh_train[index]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "permanent-present",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å•è¯cucumberå¯¹åº”çš„ç´¢å¼•æ˜¯ï¼š113317\n",
      "ç´¢å¼•113317å¯¹åº”çš„å•è¯æ˜¯ï¼šcucumber\n"
     ]
    }
   ],
   "source": [
    "word_to_index, index_to_word, word_to_vec_map = emo_utils.read_glove_vecs('data/glove.6B.50d.txt')\n",
    "#word_to_indexï¼šå­—å…¸ç±»å‹çš„è¯æ±‡ï¼ˆ400,001ä¸ªï¼‰ä¸ç´¢å¼•çš„æ˜ å°„ï¼ˆæœ‰æ•ˆèŒƒå›´ï¼š0-400,000ï¼‰\n",
    "#index_to_wordï¼šå­—å…¸ç±»å‹çš„ç´¢å¼•ä¸è¯æ±‡ä¹‹é—´çš„æ˜ å°„ã€‚\n",
    "#word_to_vec_mapï¼šå­—å…¸ç±»å‹çš„è¯æ±‡ä¸å¯¹åº”GloVeå‘é‡çš„æ˜ å°„ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "raising-uruguay",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1st step: transform and average\n",
    "def sentence_to_avg(sentence, word_to_vec_map):\n",
    "    \"\"\"\n",
    "    å°†å¥å­è½¬æ¢ä¸ºå•è¯åˆ—è¡¨ï¼Œæå–å…¶GloVeå‘é‡ï¼Œç„¶åå°†å…¶å¹³å‡ã€‚\n",
    "    \n",
    "    å‚æ•°ï¼š\n",
    "        sentence -- å­—ç¬¦ä¸²ç±»å‹ï¼Œä»Xä¸­è·å–çš„æ ·æœ¬ã€‚\n",
    "        word_to_vec_map -- å­—å…¸ç±»å‹ï¼Œå•è¯æ˜ å°„åˆ°50ç»´çš„å‘é‡çš„å­—å…¸\n",
    "        \n",
    "    è¿”å›ï¼š\n",
    "        avg -- å¯¹å¥å­çš„å‡å€¼ç¼–ç ï¼Œç»´åº¦ä¸º(50,)\n",
    "    \"\"\"\n",
    "    #è½¬åŒ–ä¸ºå°å†™,å¹¶åˆ†å‰²å¥å­\n",
    "    words = sentence.lower().split()\n",
    "    \n",
    "    #åˆå§‹åŒ–average\n",
    "    avg = np.zeros(50,)\n",
    "    \n",
    "    #å¯¹sentenceä¸­çš„æ‰€æœ‰å•è¯æ±‚å¹³å‡\n",
    "    for w in words:\n",
    "        avg += word_to_vec_map[w]\n",
    "    avg = np.divide(avg, len(words))\n",
    "    \n",
    "    return avg\n",
    "\n",
    "#test\n",
    "# avg = sentence_to_avg(\"Morrocan couscous is my favorite dish\", word_to_vec_map)\n",
    "# print(\"avg = \", avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "engaging-potential",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ç¬¬0è½®, æŸå¤±ä¸º1.9520498812810072\n",
      "Accuracy: 0.3484848484848485\n",
      "ç¬¬100è½®, æŸå¤±ä¸º0.07971818726014807\n",
      "Accuracy: 0.9318181818181818\n",
      "ç¬¬200è½®, æŸå¤±ä¸º0.04456369243681402\n",
      "Accuracy: 0.9545454545454546\n",
      "ç¬¬300è½®, æŸå¤±ä¸º0.03432267378786059\n",
      "Accuracy: 0.9696969696969697\n"
     ]
    }
   ],
   "source": [
    "#2nd step: train and optimize\n",
    "def model(X, Y, word_to_vec_map, learning_rate=0.01, num_iterations=400):\n",
    "    \"\"\"\n",
    "    åœ¨numpyä¸­è®­ç»ƒè¯å‘é‡æ¨¡å‹ã€‚\n",
    "    \n",
    "    å‚æ•°ï¼š\n",
    "        X -- è¾“å…¥çš„å­—ç¬¦ä¸²ç±»å‹çš„æ•°æ®ï¼Œç»´åº¦ä¸º(m, 1)ã€‚\n",
    "        Y -- å¯¹åº”çš„æ ‡ç­¾ï¼Œ0-7çš„æ•°ç»„ï¼Œç»´åº¦ä¸º(m, 1)ã€‚\n",
    "        word_to_vec_map -- å­—å…¸ç±»å‹çš„å•è¯åˆ°50ç»´è¯å‘é‡çš„æ˜ å°„ã€‚\n",
    "        learning_rate -- å­¦ä¹ ç‡.\n",
    "        num_iterations -- è¿­ä»£æ¬¡æ•°ã€‚\n",
    "        \n",
    "    è¿”å›ï¼š\n",
    "        pred -- é¢„æµ‹çš„å‘é‡ï¼Œç»´åº¦ä¸º(m, 1)ã€‚\n",
    "        W -- æƒé‡å‚æ•°ï¼Œç»´åº¦ä¸º(n_y, n_h)ã€‚\n",
    "        b -- åç½®å‚æ•°ï¼Œç»´åº¦ä¸º(n_y,)\n",
    "    \"\"\"\n",
    "    np.random.seed(1)\n",
    "    \n",
    "    #å®šä¹‰è®­ç»ƒæ•°é‡\n",
    "    m = Y.shape[0]\n",
    "    n_y = 5\n",
    "    n_h = 50\n",
    "    \n",
    "    #ä½¿ç”¨Xavieråˆå§‹åŒ–å‚æ•°\n",
    "    W = np.random.randn(n_y, n_h) / np.sqrt(n_h)\n",
    "    b = np.zeros((n_y,))\n",
    "    \n",
    "    #å°†Yè½¬åŒ–æˆç‹¬çƒ­ç¼–ç \n",
    "    Y_oh = emo_utils.convert_to_one_hot(Y, C=n_y)\n",
    "    \n",
    "    #ä¼˜åŒ–å¾ªç¯\n",
    "    for t in range(num_iterations):\n",
    "        for i in range(m):\n",
    "            #å¯¹ç¬¬iä¸ªæ ·æœ¬è¿è¡ŒModel\n",
    "            avg = sentence_to_avg(X[i],word_to_vec_map)\n",
    "        \n",
    "            #forward propagation\n",
    "            z = np.dot(W, avg) + b\n",
    "            a = emo_utils.softmax(z)\n",
    "        \n",
    "            #è®¡ç®—æŸå¤±\n",
    "            cost = -np.sum(Y_oh[i]*np.log(a))\n",
    "        \n",
    "            #backward propagation\n",
    "            dz = a - Y_oh[i]\n",
    "            dW = np.dot(dz.reshape(n_y,1),avg.reshape(1,n_h)) #åˆå§‹åŒ–çš„dz, avgæ˜¯rank=1çš„,è¦è½¬åŒ–ä¸ºçŸ©é˜µ\n",
    "            db = dz\n",
    "        \n",
    "            #optimize\n",
    "            W = W - learning_rate * dW\n",
    "            b = b - learning_rate * db\n",
    "            \n",
    "        if t % 100 == 0:\n",
    "            print(\"ç¬¬{t}è½®, æŸå¤±ä¸º{cost}\".format(t=t,cost=cost))\n",
    "            pred = emo_utils.predict(X, Y, W, b, word_to_vec_map)\n",
    "    \n",
    "    return pred, W, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "funky-procurement",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ç¬¬0è½®, æŸå¤±ä¸º1.9520498812810072\n",
      "Accuracy: 0.3484848484848485\n",
      "ç¬¬100è½®, æŸå¤±ä¸º0.07971818726014807\n",
      "Accuracy: 0.9318181818181818\n",
      "ç¬¬200è½®, æŸå¤±ä¸º0.04456369243681402\n",
      "Accuracy: 0.9545454545454546\n",
      "ç¬¬300è½®, æŸå¤±ä¸º0.03432267378786059\n",
      "Accuracy: 0.9696969696969697\n",
      "=====è®­ç»ƒé›†====\n",
      "Accuracy: 0.9772727272727273\n",
      "=====æµ‹è¯•é›†====\n",
      "Accuracy: 0.8571428571428571\n"
     ]
    }
   ],
   "source": [
    "#test\n",
    "pred, W, b = model(X_train, Y_train, word_to_vec_map)\n",
    "print(\"=====è®­ç»ƒé›†====\")\n",
    "pred_train = emo_utils.predict(X_train, Y_train, W, b, word_to_vec_map)\n",
    "print(\"=====æµ‹è¯•é›†====\")\n",
    "pred_test = emo_utils.predict(X_test, Y_test, W, b, word_to_vec_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "unsigned-remainder",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8333333333333334\n",
      "\n",
      "i adore you â¤ï¸\n",
      "i love you â¤ï¸\n",
      "funny lol ğŸ˜„\n",
      "lets play with a ball âš¾\n",
      "food is ready ğŸ´\n",
      "you are not happy â¤ï¸\n"
     ]
    }
   ],
   "source": [
    "#test on my_senteces\n",
    "X_my_sentences = np.array([\"i adore you\", \"i love you\", \"funny lol\", \"lets play with a ball\", \"food is ready\", \"you are not happy\"])\n",
    "Y_my_labels = np.array([[0], [0], [2], [1], [4],[3]])\n",
    "\n",
    "pred = emo_utils.predict(X_my_sentences, Y_my_labels , W, b, word_to_vec_map)\n",
    "emo_utils.print_predictions(X_my_sentences, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "noble-applicant",
   "metadata": {},
   "source": [
    "å³ä½¿ä½ åªæœ‰128ä¸ªè®­ç»ƒæ ·æœ¬ï¼Œä½ ä¹Ÿå¯ä»¥å¾—åˆ°å¾ˆå¥½åœ°è¡¨æƒ…ç¬¦å·æ¨¡å‹ï¼Œå› ä¸ºè¯å‘é‡æ˜¯è®­ç»ƒå¥½äº†çš„ï¼Œå®ƒä¼šç»™ä½ ä¸€ä¸ªè¾ƒå¥½çš„æ¦‚æ‹¬èƒ½åŠ›ã€‚\n",
    "\n",
    "Emojifier-V1æ˜¯æœ‰ç¼ºé™·çš„ï¼Œæ¯”å¦‚å®ƒä¸ä¼šæŠŠâ€œThis movie is not good and not enjoyableâ€åˆ’åˆ†ä¸ºä¸å¥½ä¸€ç±»ï¼Œå› ä¸ºå®ƒåªæ˜¯å°†æ‰€æœ‰å•è¯çš„å‘é‡åšäº†å¹³å‡ï¼Œæ²¡æœ‰å…³å¿ƒè¿‡é¡ºåºã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "after-combine",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "C:\\Users\\ersac\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\ersac\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\ersac\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\ersac\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\ersac\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\ersac\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\Users\\ersac\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\ersac\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\ersac\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\ersac\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\ersac\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\ersac\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "import keras\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Input, Dropout, LSTM, Activation\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "np.random.seed(1)\n",
    "from keras.initializers import glorot_uniform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "million-sunglasses",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentences_to_indices(X, word_to_index, max_len):\n",
    "    \"\"\"\n",
    "    è¾“å…¥çš„æ˜¯Xï¼ˆå­—ç¬¦ä¸²ç±»å‹çš„å¥å­çš„æ•°ç»„ï¼‰ï¼Œå†è½¬åŒ–ä¸ºå¯¹åº”çš„å¥å­åˆ—è¡¨ï¼Œ\n",
    "    è¾“å‡ºçš„æ˜¯èƒ½å¤Ÿè®©Embedding()å‡½æ•°æ¥å—çš„åˆ—è¡¨æˆ–çŸ©é˜µï¼ˆå‚è§å›¾4ï¼‰ã€‚\n",
    "    \n",
    "    å‚æ•°ï¼š\n",
    "        X -- å¥å­æ•°ç»„ï¼Œç»´åº¦ä¸º(m, 1)\n",
    "        word_to_index -- å­—å…¸ç±»å‹çš„å•è¯åˆ°ç´¢å¼•çš„æ˜ å°„\n",
    "        max_len -- æœ€å¤§å¥å­çš„é•¿åº¦ï¼Œæ•°æ®é›†ä¸­æ‰€æœ‰çš„å¥å­çš„é•¿åº¦éƒ½ä¸ä¼šè¶…è¿‡å®ƒã€‚\n",
    "        \n",
    "    è¿”å›ï¼š\n",
    "        X_indices -- å¯¹åº”äºXä¸­çš„å•è¯ç´¢å¼•æ•°ç»„ï¼Œç»´åº¦ä¸º(m, max_len)\n",
    "    \"\"\"\n",
    "    m = X.shape[0]\n",
    "    X_indices = np.zeros((m, max_len))\n",
    "    \n",
    "    for i in range(m):\n",
    "        j = 0\n",
    "        sentence_words = X[i].lower().split()\n",
    "        for w in sentence_words:\n",
    "            X_indices[i][j] = word_to_index[w]\n",
    "            j += 1\n",
    "    \n",
    "    return X_indices\n",
    "\n",
    "#test\n",
    "# X1 = np.array([\"funny lol\", \"lets play baseball\", \"food is ready for you\"])\n",
    "# X1_indices = sentences_to_indices(X1,word_to_index, max_len = 5)\n",
    "# print(\"X1 =\", X1)\n",
    "# print(\"X1_indices =\", X1_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "convinced-smooth",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights[0][1][3] = -0.3403\n"
     ]
    }
   ],
   "source": [
    "def pretrained_embedding_layer(word_to_vec_map, word_to_index):\n",
    "    \"\"\"\n",
    "    åˆ›å»ºKeras Embedding()å±‚ï¼ŒåŠ è½½å·²ç»è®­ç»ƒå¥½äº†çš„50ç»´GloVeå‘é‡\n",
    "    \n",
    "    å‚æ•°ï¼š\n",
    "        word_to_vec_map -- å­—å…¸ç±»å‹çš„å•è¯ä¸è¯åµŒå…¥çš„æ˜ å°„\n",
    "        word_to_index -- å­—å…¸ç±»å‹çš„å•è¯åˆ°è¯æ±‡è¡¨ï¼ˆ400,001ä¸ªå•è¯ï¼‰çš„ç´¢å¼•çš„æ˜ å°„ã€‚\n",
    "        \n",
    "    è¿”å›ï¼š\n",
    "        embedding_layer() -- è®­ç»ƒå¥½äº†çš„Kerasçš„å®ä½“å±‚ã€‚\n",
    "    \"\"\"\n",
    "    vocab_len = len(word_to_index) + 1\n",
    "    emb_dim = word_to_vec_map[\"cucumber\"].shape[0]\n",
    "    \n",
    "    #åˆå§‹åŒ–åµŒå…¥çŸ©é˜µ\n",
    "    emb_matrix = np.zeros((vocab_len, emb_dim))\n",
    "    \n",
    "    #è®²åµŒå…¥çŸ©é˜µçš„æ¯è¡Œçš„â€œindexâ€è®¾ç½®ä¸ºè¯æ±‡\"index\"çš„è¯å‘é‡è¡¨ç¤º\n",
    "    for word, index in word_to_index.items():\n",
    "        emb_matrix[index,:] = word_to_vec_map[word]\n",
    "        \n",
    "    #å®šä¹‰Kerasçš„embeddingå±‚\n",
    "    embedding_layer = Embedding(vocab_len, emb_dim, trainable=False)\n",
    "    \n",
    "    #æ„å»ºembeddingå±‚\n",
    "    embedding_layer.build((None,))\n",
    "    \n",
    "    #è®²åµŒå…¥å±‚çš„æƒé‡è®¾ç½®ä¸ºåµŒå…¥çŸ©é˜µ\n",
    "    embedding_layer.set_weights([emb_matrix])\n",
    "    \n",
    "    return embedding_layer\n",
    "\n",
    "#test\n",
    "# embedding_layer = pretrained_embedding_layer(word_to_vec_map, word_to_index)\n",
    "# print(\"weights[0][1][3] =\", embedding_layer.get_weights()[0][1][3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "artistic-strap",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Emojify_V2(input_shape, word_to_vec_map, word_to_index):\n",
    "    \"\"\"\n",
    "    å®ç°Emojify-V2æ¨¡å‹çš„è®¡ç®—å›¾\n",
    "    \n",
    "    å‚æ•°ï¼š\n",
    "        input_shape -- è¾“å…¥çš„ç»´åº¦ï¼Œé€šå¸¸æ˜¯(max_len,)\n",
    "        word_to_vec_map -- å­—å…¸ç±»å‹çš„å•è¯ä¸è¯åµŒå…¥çš„æ˜ å°„ã€‚\n",
    "        word_to_index -- å­—å…¸ç±»å‹çš„å•è¯åˆ°è¯æ±‡è¡¨ï¼ˆ400,001ä¸ªå•è¯ï¼‰çš„ç´¢å¼•çš„æ˜ å°„ã€‚\n",
    "    \n",
    "    è¿”å›ï¼š\n",
    "        model -- Kerasæ¨¡å‹å®ä½“\n",
    "    \"\"\"\n",
    "    #å®šä¹‰sentece_indicesä¸ºè®¡ç®—å›¾çš„è¾“å…¥ï¼Œç»´åº¦ä¸º(input_shape,),ç±»å‹ä¸ºdtype\"int32\"\n",
    "    sentence_indices = Input(input_shape, dtype=\"int32\")\n",
    "    \n",
    "    #åˆ›å»ºembeddingå±‚\n",
    "    embedding_layer = pretrained_embedding_layer(word_to_vec_map, word_to_index)\n",
    "    \n",
    "    #é€šè¿‡åµŒå…¥å±‚ä¼ æ’­sentence_indicesï¼Œå¾—åˆ°åµŒå…¥çš„ç»“æœ\n",
    "    embeddings = embedding_layer(sentence_indices)\n",
    "    \n",
    "    #é€šè¿‡å¸¦æœ‰128ç»´éšè—çŠ¶æ€çš„LSTMå±‚ä¼ æ’­åµŒå…¥\n",
    "    #éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œè¿™é‡Œæ˜¯å›¾å½¢çš„ç¬¬ä¸€å±‚ï¼Œè¿”å›çš„è¾“å‡ºåº”è¯¥æ˜¯ä¸€æ‰¹åºåˆ—\n",
    "    X = LSTM(128, return_sequences=True)(embeddings)\n",
    "    #ä½¿ç”¨dropout\n",
    "    X = Dropout(0.5)(X)\n",
    "    #é€šè¿‡ç¬¬äºŒå±‚128ç»´éšè—çŠ¶æ€çš„LSTMå±‚ä¼ æ’­X\n",
    "    #è¿™æ—¶æ˜¯many-to-oneæ¨¡å‹ï¼Œè¾“å‡ºæ˜¯æœ€åä¸€ä¸ªy\n",
    "    X = LSTM(128, return_sequences=False)(X)\n",
    "    #ä½¿ç”¨dropout\n",
    "    X = Dropout(0.5)(X)\n",
    "    #Denseå±‚å…¨è¿æ¥\n",
    "    X = Dense(5)(X)\n",
    "    #æ·»åŠ softmaxæ¿€æ´»\n",
    "    X = Activation(\"softmax\")(X)\n",
    "    \n",
    "    #åˆ›å»ºæ¨¡å‹å®ä½“\n",
    "    model = Model(inputs=sentence_indices, outputs=X)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "moved-title",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "embedding_2 (Embedding)      (None, 10, 50)            20000050  \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 10, 128)           91648     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 10, 128)           0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 128)               131584    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 5)                 645       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 5)                 0         \n",
      "=================================================================\n",
      "Total params: 20,223,927\n",
      "Trainable params: 223,877\n",
      "Non-trainable params: 20,000,050\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Emojify_V2((10,), word_to_vec_map, word_to_index)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "female-character",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ç¼–è¯‘æ¨¡å‹\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "threaded-korea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\ersac\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Epoch 1/50\n",
      "132/132 [==============================] - 2s 18ms/step - loss: 1.6100 - accuracy: 0.1970\n",
      "Epoch 2/50\n",
      "132/132 [==============================] - 0s 2ms/step - loss: 1.5254 - accuracy: 0.3030\n",
      "Epoch 3/50\n",
      "132/132 [==============================] - 0s 2ms/step - loss: 1.4885 - accuracy: 0.3182\n",
      "Epoch 4/50\n",
      "132/132 [==============================] - 0s 2ms/step - loss: 1.4148 - accuracy: 0.3939\n",
      "Epoch 5/50\n",
      "132/132 [==============================] - 0s 2ms/step - loss: 1.3107 - accuracy: 0.4773\n",
      "Epoch 6/50\n",
      "132/132 [==============================] - 0s 2ms/step - loss: 1.1912 - accuracy: 0.5455\n",
      "Epoch 7/50\n",
      "132/132 [==============================] - 0s 3ms/step - loss: 1.1405 - accuracy: 0.5000\n",
      "Epoch 8/50\n",
      "132/132 [==============================] - 0s 2ms/step - loss: 0.9976 - accuracy: 0.6288\n",
      "Epoch 9/50\n",
      "132/132 [==============================] - 0s 2ms/step - loss: 0.8248 - accuracy: 0.7424\n",
      "Epoch 10/50\n",
      "132/132 [==============================] - 0s 2ms/step - loss: 0.7721 - accuracy: 0.6970\n",
      "Epoch 11/50\n",
      "132/132 [==============================] - 0s 2ms/step - loss: 0.6461 - accuracy: 0.7727\n",
      "Epoch 12/50\n",
      "132/132 [==============================] - 0s 2ms/step - loss: 0.5790 - accuracy: 0.8030\n",
      "Epoch 13/50\n",
      "132/132 [==============================] - 0s 2ms/step - loss: 0.4678 - accuracy: 0.8485\n",
      "Epoch 14/50\n",
      "132/132 [==============================] - 0s 2ms/step - loss: 0.4914 - accuracy: 0.8106\n",
      "Epoch 15/50\n",
      "132/132 [==============================] - 0s 2ms/step - loss: 0.4647 - accuracy: 0.8258\n",
      "Epoch 16/50\n",
      "132/132 [==============================] - 0s 2ms/step - loss: 0.3228 - accuracy: 0.8712\n",
      "Epoch 17/50\n",
      "132/132 [==============================] - 0s 2ms/step - loss: 0.3557 - accuracy: 0.8864\n",
      "Epoch 18/50\n",
      "132/132 [==============================] - 0s 2ms/step - loss: 0.5724 - accuracy: 0.8485\n",
      "Epoch 19/50\n",
      "132/132 [==============================] - 0s 2ms/step - loss: 0.4483 - accuracy: 0.8333\n",
      "Epoch 20/50\n",
      "132/132 [==============================] - 0s 2ms/step - loss: 0.3570 - accuracy: 0.8788\n",
      "Epoch 21/50\n",
      "132/132 [==============================] - 0s 2ms/step - loss: 0.3733 - accuracy: 0.8258\n",
      "Epoch 22/50\n",
      "132/132 [==============================] - 0s 2ms/step - loss: 0.3615 - accuracy: 0.8712\n",
      "Epoch 23/50\n",
      "132/132 [==============================] - 0s 2ms/step - loss: 0.2990 - accuracy: 0.8712\n",
      "Epoch 24/50\n",
      "132/132 [==============================] - 0s 2ms/step - loss: 0.3387 - accuracy: 0.8864\n",
      "Epoch 25/50\n",
      "132/132 [==============================] - 0s 2ms/step - loss: 0.2430 - accuracy: 0.9394\n",
      "Epoch 26/50\n",
      "132/132 [==============================] - 0s 3ms/step - loss: 0.1867 - accuracy: 0.9470\n",
      "Epoch 27/50\n",
      "132/132 [==============================] - 0s 2ms/step - loss: 0.1421 - accuracy: 0.9697\n",
      "Epoch 28/50\n",
      "132/132 [==============================] - 0s 2ms/step - loss: 0.1502 - accuracy: 0.9621\n",
      "Epoch 29/50\n",
      "132/132 [==============================] - 0s 2ms/step - loss: 0.1043 - accuracy: 0.9773\n",
      "Epoch 30/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.1094 - accuracy: 0.9621\n",
      "Epoch 31/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.1593 - accuracy: 0.9470\n",
      "Epoch 32/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.1200 - accuracy: 0.9621\n",
      "Epoch 33/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.0808 - accuracy: 0.9848\n",
      "Epoch 34/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.1158 - accuracy: 0.9773\n",
      "Epoch 35/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.2279 - accuracy: 0.9167\n",
      "Epoch 36/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.0828 - accuracy: 0.9773\n",
      "Epoch 37/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.2213 - accuracy: 0.9242\n",
      "Epoch 38/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.1083 - accuracy: 0.9470\n",
      "Epoch 39/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.1416 - accuracy: 0.9470\n",
      "Epoch 40/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.1460 - accuracy: 0.9545\n",
      "Epoch 41/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.0966 - accuracy: 0.9621\n",
      "Epoch 42/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.1102 - accuracy: 0.9697\n",
      "Epoch 43/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.0682 - accuracy: 0.9773\n",
      "Epoch 44/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.0681 - accuracy: 0.9848\n",
      "Epoch 45/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.3102 - accuracy: 0.8939\n",
      "Epoch 46/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.3442 - accuracy: 0.8712\n",
      "Epoch 47/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.3552 - accuracy: 0.8864\n",
      "Epoch 48/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.1654 - accuracy: 0.9545\n",
      "Epoch 49/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.2186 - accuracy: 0.9167\n",
      "Epoch 50/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.0945 - accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x2c1127358d0>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#è®­ç»ƒé›†ä¸Šçš„ç»“æœ\n",
    "X_train_indices = sentences_to_indices(X_train, word_to_index, 10)\n",
    "Y_train_oh = emo_utils.convert_to_one_hot(Y_train, C = 5)\n",
    "model.fit(X_train_indices, Y_train_oh, epochs = 50, batch_size = 32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "paperback-fitness",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56/56 [==============================] - 0s 7ms/step\n",
      "Test accuracy =  0.8392857313156128\n"
     ]
    }
   ],
   "source": [
    "#æµ‹è¯•é›†ä¸Šçš„ç»“æœ\n",
    "X_test_indices = sentences_to_indices(X_test, word_to_index, max_len = 10)\n",
    "Y_test_oh = emo_utils.convert_to_one_hot(Y_test, C = 5)\n",
    "loss, acc = model.evaluate(X_test_indices, Y_test_oh)\n",
    "\n",
    "print(\"Test accuracy = \", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "sweet-example",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ­£ç¡®è¡¨æƒ…ï¼šğŸ˜„   é¢„æµ‹ç»“æœï¼š he got a very nice raise\tâ¤ï¸\n",
      "æ­£ç¡®è¡¨æƒ…ï¼šğŸ˜„   é¢„æµ‹ç»“æœï¼š she got me a nice present\tâ¤ï¸\n",
      "æ­£ç¡®è¡¨æƒ…ï¼šğŸ˜„   é¢„æµ‹ç»“æœï¼š he is a good friend\tâ¤ï¸\n",
      "æ­£ç¡®è¡¨æƒ…ï¼šğŸ˜   é¢„æµ‹ç»“æœï¼š work is hard\tğŸ˜„\n",
      "æ­£ç¡®è¡¨æƒ…ï¼šğŸ˜   é¢„æµ‹ç»“æœï¼š This girl is messing with me\tâ¤ï¸\n",
      "æ­£ç¡®è¡¨æƒ…ï¼šâ¤ï¸   é¢„æµ‹ç»“æœï¼š I love taking breaks\tğŸ˜\n",
      "æ­£ç¡®è¡¨æƒ…ï¼šğŸ˜„   é¢„æµ‹ç»“æœï¼š you brighten my day\tâ¤ï¸\n",
      "æ­£ç¡®è¡¨æƒ…ï¼šğŸ˜„   é¢„æµ‹ç»“æœï¼š will you be my valentine\tâ¤ï¸\n",
      "æ­£ç¡®è¡¨æƒ…ï¼šğŸ˜„   é¢„æµ‹ç»“æœï¼š I like to laugh\tâ¤ï¸\n"
     ]
    }
   ],
   "source": [
    "#æŸ¥çœ‹é”™è¯¯ä¾‹å­\n",
    "C = 5\n",
    "y_test_oh = np.eye(C)[Y_test.reshape(-1)]\n",
    "X_test_indices = sentences_to_indices(X_test, word_to_index, 10)\n",
    "pred = model.predict(X_test_indices)\n",
    "for i in range(len(X_test)):\n",
    "    x = X_test_indices\n",
    "    num = np.argmax(pred[i])\n",
    "    if(num != Y_test[i]):\n",
    "        print('æ­£ç¡®è¡¨æƒ…ï¼š'+ emo_utils.label_to_emoji(Y_test[i]) + '   é¢„æµ‹ç»“æœï¼š '+ X_test[i] + emo_utils.label_to_emoji(num).strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "express-timer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "you are not happy ğŸ˜\n"
     ]
    }
   ],
   "source": [
    "#æµ‹è¯•my sentences\n",
    "x_test = np.array(['you are not happy'])\n",
    "X_test_indices = sentences_to_indices(x_test, word_to_index, maxLen)\n",
    "print(x_test[0] +' '+  emo_utils.label_to_emoji(np.argmax(model.predict(X_test_indices))))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
