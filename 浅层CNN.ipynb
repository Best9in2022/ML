{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "digital-certificate",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (5.0, 4.0) \n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "#ipython很好用，但是如果在ipython里已经import过的模块修改后需要重新reload就需要这样\n",
    "#在执行用户代码前，重新装入软件的扩展和模块。\n",
    "%load_ext autoreload   \n",
    "#autoreload 2：装入所有 %aimport 不包含的模块。\n",
    "%autoreload 2          \n",
    "\n",
    "np.random.seed(1)      #指定随机种子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "accepting-siemens",
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_pad(X,pad):\n",
    "    \"\"\"\n",
    "    把数据集X的图像边界全部使用0来扩充pad个宽度和高度。\n",
    "    \n",
    "    参数：\n",
    "        X - 图像数据集，维度为（样本数，图像高度，图像宽度，图像通道数）\n",
    "        pad - 整数，每个图像在垂直和水平维度上的填充量\n",
    "    返回：\n",
    "        X_paded - 扩充后的图像数据集，维度为（样本数，图像高度 + 2*pad，图像宽度 + 2*pad，图像通道数）\n",
    "    \n",
    "    \"\"\"\n",
    "    X_paded = np.pad(X,(\n",
    "                        (0,0),     #样本数不填充\n",
    "                        (pad,pad), #图像高度，上/下面都填充pad个(x,y)\n",
    "                        (pad,pad), #图像宽度，上/下面都填充pad个(x,y)\n",
    "                        (0,0))     #图像轨道数，不填充\n",
    "                        ,'constant', constant_values=0)    #填充方式为constant,填充值为0\n",
    "    \n",
    "    return X_paded\n",
    "\n",
    "#test\n",
    "# np.random.seed(1)\n",
    "# x = np.random.randn(4,3,3,2)\n",
    "# x_paded = zero_pad(x,2)\n",
    "# #查看信息\n",
    "# print (\"x.shape =\", x.shape)\n",
    "# print (\"x_paded.shape =\", x_paded.shape)\n",
    "# print (\"x[1, 1] =\", x[1, 1])\n",
    "# print (\"x_paded[1, 1] =\", x_paded[1, 1])\n",
    "\n",
    "# #绘制图\n",
    "# fig , axarr = plt.subplots(1,2)  #一行两列\n",
    "# axarr[0].set_title('x')\n",
    "# axarr[0].imshow(x[0,:,:,0])\n",
    "# axarr[1].set_title('x_paded')\n",
    "# axarr[1].imshow(x_paded[0,:,:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "communist-chuck",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_single_step(a_slice_prev,W,b):\n",
    "    \"\"\"\n",
    "    在前一层的激活输出的一个片段上应用一个由参数W定义的过滤器。\n",
    "    这里切片大小和过滤器大小相同\n",
    "    \n",
    "    参数：\n",
    "        a_slice_prev - 输入数据的一个片段，维度为（过滤器大小，过滤器大小，上一通道数）\n",
    "        W - 权重参数，包含在了一个矩阵中，维度为（过滤器大小，过滤器大小，上一通道数）\n",
    "        b - 偏置参数，包含在了一个矩阵中，维度为（1,1,1）\n",
    "        \n",
    "    返回：\n",
    "        Z - 在输入数据的片X上卷积滑动窗口（w，b）的结果。\n",
    "    \"\"\"\n",
    "    s = np.multiply(a_slice_prev,W) + b\n",
    "    Z = np.sum(s)\n",
    "    \n",
    "    return Z\n",
    "#test\n",
    "# np.random.seed(1)\n",
    "\n",
    "#这里切片大小和过滤器大小相同\n",
    "# a_slice_prev = np.random.randn(4,4,3)\n",
    "# W = np.random.randn(4,4,3)\n",
    "# b = np.random.randn(1,1,1)\n",
    "\n",
    "# Z = conv_single_step(a_slice_prev,W,b)\n",
    "\n",
    "# print(\"Z = \" + str(Z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "magnetic-marshall",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_forward(A_prev, W, b, hparameters):\n",
    "    \"\"\"\n",
    "    实现卷积函数的前向传播\n",
    "    \n",
    "    参数：\n",
    "        A_prev - 上一层的激活输出矩阵，维度为(m, n_H_prev, n_W_prev, n_C_prev)，（样本数量，上一层图像的高度，上一层图像的宽度，上一层过滤器数量）\n",
    "        W - 权重矩阵，维度为(f, f, n_C_prev, n_C)，（过滤器大小，过滤器大小，上一层的过滤器数量，这一层的过滤器数量）\n",
    "        b - 偏置矩阵，维度为(1, 1, 1, n_C)，（1,1,1,这一层的过滤器数量）\n",
    "        hparameters - 包含了\"stride\"与 \"pad\"的超参数字典。\n",
    "    \n",
    "    返回：\n",
    "        Z - 卷积输出，维度为(m, n_H, n_W, n_C)，（样本数，图像的高度，图像的宽度，过滤器数量）\n",
    "        cache - 缓存了一些反向传播函数conv_backward()需要的一些数据\n",
    "    \"\"\"\n",
    "    \n",
    "    #获取来自上一层的基本信息\n",
    "    (m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape\n",
    "    \n",
    "    #获取权重的基本信息\n",
    "    (f, f, n_C_prev, n_C) = W.shape\n",
    "    \n",
    "    #获取超参数parameters的值\n",
    "    stride = hparameters[\"stride\"]\n",
    "    pad = hparameters[\"pad\"]\n",
    "    \n",
    "    #计算卷积后的图像的宽度高度，使用int来进行板除\n",
    "    n_H = int((n_H_prev + 2* pad - f) / stride) + 1\n",
    "    n_W = int((n_W_prev + 2* pad - f) / stride) + 1\n",
    "    \n",
    "    #使用0来初始化卷积的输出\n",
    "    Z = np.zeros((m,n_H,n_W,n_C))\n",
    "    \n",
    "    #通过A_prev创建填充了的A_prev_pad\n",
    "    A_prev_pad = zero_pad(A_prev,pad)\n",
    "    \n",
    "    for i in range(m):                         #遍历样本\n",
    "        a_prev_pad = A_prev_pad[i]             #选择填充过后的第i个激活矩阵，第i个样本\n",
    "        for h in range(n_H):                   #图片的高度，不是n_H_prev，不用考虑参数的问题了（已经知道遍历的下界是多少了）\n",
    "             #3rd->纵向移动   \n",
    "            for w in range(n_W):               #图片的宽度\n",
    "                #2nd->横向移动\n",
    "                for c in range(n_C):           #filter的个数\n",
    "                    #1st->先算filter\n",
    "                    #计算切片的范围\n",
    "                    vert_start = h*stride      #竖向开始的位置\n",
    "                    vert_end   = vert_start + f#竖向结束的位置\n",
    "                    horiz_start= w*stride      #纵向开始的位置\n",
    "                    horiz_end  = horiz_start + f#纵向结束的位置\n",
    "                    \n",
    "                    #取出切片，第三个维度全部都要取，对应n_C_prev\n",
    "                    a_slice_prev = a_prev_pad[vert_start:vert_end,horiz_start:horiz_end,:]  \n",
    "                    \n",
    "                    #执行单步卷积，W的第四个维度对应filter的编号，对应Z的第四个维度\n",
    "                    Z[i,h,w,c] = conv_single_step(a_slice_prev,W[:,:,:,c],b[0,0,0,c])\n",
    "                    \n",
    "    #数据处理完毕，检验\n",
    "    assert(Z.shape == (m,n_H,n_W,n_C))\n",
    "    \n",
    "    #储存一些缓存值\n",
    "    cache = (A_prev,W,b,hparameters)\n",
    "    \n",
    "    return (Z, cache)\n",
    "\n",
    "#test\n",
    "# np.random.seed(1)\n",
    "\n",
    "# A_prev = np.random.randn(10,4,4,3)\n",
    "# W = np.random.randn(2,2,3,8)\n",
    "# b = np.random.randn(1,1,1,8)\n",
    "\n",
    "# hparameters = {\"pad\" : 2, \"stride\": 1}\n",
    "\n",
    "# Z , cache_conv = conv_forward(A_prev,W,b,hparameters)\n",
    "\n",
    "# print(\"np.mean(Z) = \", np.mean(Z))\n",
    "# print(\"cache_conv[0][1][2][3] =\", cache_conv[0][1][2][3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "packed-highway",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pool_forward(A_prev,hparameters,mode=\"max\"):\n",
    "    \"\"\"\n",
    "    实现池化层的前向传播\n",
    "    \n",
    "    参数：\n",
    "        A_prev - 输入数据，维度为(m, n_H_prev, n_W_prev, n_C_prev)\n",
    "        hparameters - 包含了 \"f\" 和 \"stride\"的超参数字典\n",
    "        mode - 模式选择【\"max\" | \"average\"】\n",
    "        \n",
    "    返回：\n",
    "        A - 池化层的输出，维度为 (m, n_H, n_W, n_C)\n",
    "        cache - 存储了一些反向传播需要用到的值，包含了输入和超参数的字典。\n",
    "    \"\"\"\n",
    "    (m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape\n",
    "    stride = hparameters[\"stride\"]\n",
    "    f = hparameters[\"f\"]\n",
    "    \n",
    "    n_H = int((n_H_prev - f)/ stride) + 1\n",
    "    n_W = int((n_W_prev - f)/ stride) + 1\n",
    "    n_C = n_C_prev\n",
    "    \n",
    "    A = np.zeros((m,n_H,n_W,n_C))\n",
    "    \n",
    "    for i in range(m):\n",
    "        for h in range(n_H):\n",
    "            for w in range(n_W):\n",
    "                vert_start = h * stride\n",
    "                vert_end   = vert_start + f\n",
    "                horiz_start= w * stride\n",
    "                horiz_end  = horiz_start + f\n",
    "                for c in range(n_C):\n",
    "                    if mode == \"max\":\n",
    "                        A[i, h, w, c] = np.max(A_prev[i,vert_start:vert_end,horiz_start:horiz_end,c])\n",
    "                    elif mode == \"average\":\n",
    "                        A[i, h, w, c] = np.mean(A_prev[i,vert_start:vert_end,horiz_start:horiz_end,c])\n",
    "    \n",
    "    assert(A.shape == (m, n_H, n_W, n_C))\n",
    "    \n",
    "    cache = (A_prev, hparameters)\n",
    "    \n",
    "    return A,cache\n",
    "                        \n",
    "#test\n",
    "# np.random.seed(1)\n",
    "# A_prev = np.random.randn(2,4,4,3)\n",
    "# hparameters = {\"f\":4 , \"stride\":1}\n",
    "\n",
    "# A , cache = pool_forward(A_prev,hparameters,mode=\"max\")\n",
    "# A, cache = pool_forward(A_prev, hparameters)\n",
    "# print(\"mode = max\")\n",
    "# print(\"A =\", A)\n",
    "# print(\"----------------------------\")\n",
    "# A, cache = pool_forward(A_prev, hparameters, mode = \"average\")\n",
    "# print(\"mode = average\")\n",
    "# print(\"A =\", A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "important-schema",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_backward(dZ,cache):\n",
    "    \"\"\"\n",
    "    实现卷积层的反向传播\n",
    "    \n",
    "    参数：\n",
    "        dZ - 卷积层的输出Z的 梯度，维度为(m, n_H, n_W, n_C)\n",
    "        cache - 反向传播所需要的参数，conv_forward()的输出之一\n",
    "        \n",
    "    返回：\n",
    "        dA_prev - 卷积层的输入（A_prev）的梯度值，维度为(m, n_H_prev, n_W_prev, n_C_prev)\n",
    "        dW - 卷积层的权值的梯度，维度为(f,f,n_C_prev,n_C)\n",
    "        db - 卷积层的偏置的梯度，维度为（1,1,1,n_C）\n",
    "    \n",
    "    \"\"\"\n",
    "    #获取cache的值\n",
    "    (A_prev, W, b, hparameters) = cache\n",
    "    \n",
    "    #获取信息\n",
    "    (m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape\n",
    "    \n",
    "    #获取dz的信息\n",
    "    (m, n_H, n_W, n_C) = dZ.shape\n",
    "    \n",
    "    #获取dw的信息\n",
    "    (f, f, n_C_prev, n_C) = W.shape\n",
    "    \n",
    "    #获取超参数\n",
    "    pad = hparameters[\"pad\"]\n",
    "    stride = hparameters[\"stride\"]\n",
    "    \n",
    "    #初始化各个梯度的结构\n",
    "    dA_prev = np.zeros((m, n_H_prev, n_W_prev, n_C_prev))\n",
    "    dW = np.zeros((f, f, n_C_prev, n_C))\n",
    "    db = np.zeros((1,1,1,n_C))\n",
    "    \n",
    "    #填充\n",
    "    A_prev_pad = zero_pad(A_prev,pad)\n",
    "    dA_prev_pad = zero_pad(dA_prev,pad)\n",
    "    \n",
    "    for i in range(m):\n",
    "        #选择第i个样本, 降了一个维度\n",
    "        a_prev_pad = A_prev_pad[i]\n",
    "        da_prev_pad = dA_prev_pad[i]\n",
    "    \n",
    "        #现在处理数据\n",
    "        for h in range(n_H):\n",
    "            for w in range(n_W):\n",
    "                for c in range(n_C):\n",
    "                    vert_start = h * stride\n",
    "                    vert_end   = vert_start + f\n",
    "                    horiz_start= w * stride\n",
    "                    horiz_end  = horiz_start+ f\n",
    "                \n",
    "                    a_slice = a_prev_pad[vert_start:vert_end, horiz_start:horiz_end,:]\n",
    "                    \n",
    "                    da_prev_pad[vert_start:vert_end, horiz_start:horiz_end,:] += W[:,:,:,c] * dZ[i, h, w, c]\n",
    "                    dW[:,:,:,c] += a_slice * dZ[i, h, w, c]\n",
    "                    db[:,:,:,c] += dZ[i, h, w, c]\n",
    "                  \n",
    "        #把非填充部分的数据拿出来        \n",
    "        dA_prev[i,:,:,:] = da_prev_pad[pad:-pad,pad:-pad,:]\n",
    "        \n",
    "    assert(dA_prev.shape == (m, n_H_prev, n_W_prev, n_C_prev))\n",
    "    \n",
    "    return (dA_prev, dW, db)\n",
    "\n",
    "#test\n",
    "# np.random.seed(1)\n",
    "# #初始化参数\n",
    "# A_prev = np.random.randn(10,4,4,3)\n",
    "# W = np.random.randn(2,2,3,8)\n",
    "# b = np.random.randn(1,1,1,8)\n",
    "# hparameters = {\"pad\" : 2, \"stride\": 1}\n",
    "\n",
    "# #前向传播\n",
    "# Z , cache_conv = conv_forward(A_prev,W,b,hparameters)\n",
    "# #反向传播\n",
    "# dA , dW , db = conv_backward(Z,cache_conv)\n",
    "# print(\"dA_mean =\", np.mean(dA))\n",
    "# print(\"dW_mean =\", np.mean(dW))\n",
    "# print(\"db_mean =\", np.mean(db))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "british-mainland",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mask_from_window(x):\n",
    "    \"\"\"\n",
    "    从输入矩阵中创建掩码，以保存最大值的矩阵的位置。\n",
    "    \n",
    "    参数：\n",
    "        x - 一个维度为(f,f)的矩阵\n",
    "        \n",
    "    返回：\n",
    "        mask - 包含x的最大值的位置的矩阵\n",
    "    \"\"\"\n",
    "    mask = x == np.max(x) #第一个=是赋值;第二个是等号，是判断\n",
    "    \n",
    "    return mask\n",
    "\n",
    "#test\n",
    "# np.random.seed(1)\n",
    "\n",
    "# x = np.random.randn(2,3)\n",
    "\n",
    "# mask = create_mask_from_window(x)\n",
    "\n",
    "# print(\"x = \" + str(x)) \n",
    "# print(\"mask = \" + str(mask))\\\n",
    "\n",
    "def distribute_value(dz,shape):\n",
    "    \"\"\"\n",
    "    给定一个值，为按矩阵大小平均分配到每一个矩阵位置中。\n",
    "    \n",
    "    参数：\n",
    "        dz - 输入的实数\n",
    "        shape - 元组，两个值，分别为n_H , n_W\n",
    "        \n",
    "    返回：\n",
    "        a - 已经分配好了值的矩阵，里面的值全部一样。\n",
    "    \n",
    "    \"\"\"\n",
    "    #获取矩阵大小\n",
    "    \n",
    "    #获取数值\n",
    "    average = dz / (shape[0] * shape[1])\n",
    "    \n",
    "    #填充入矩阵\n",
    "    a= np.ones(shape) * average\n",
    "    \n",
    "    return a\n",
    "\n",
    "#test\n",
    "# dz = 2\n",
    "# shape = (2,2)\n",
    "\n",
    "# a = distribute_value(dz,shape)\n",
    "# print(\"a = \" + str(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "valid-motorcycle",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pool_backward(dA,cache,mode = \"max\"):\n",
    "    \"\"\"\n",
    "    实现池化层的反向传播\n",
    "    \n",
    "    参数:\n",
    "        dA - 池化层的输出的梯度，和池化层的输出的维度一样\n",
    "        cache - 池化层前向传播时所存储的参数。\n",
    "        mode - 模式选择，【\"max\" | \"average\"】\n",
    "        \n",
    "    返回：\n",
    "        dA_prev - 池化层的输入的梯度，和A_prev的维度相同\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    #获取cache的值\n",
    "    (A_prev, hparameters) = cache\n",
    "    \n",
    "    #获取parameters的值\n",
    "    f = hparameters[\"f\"]\n",
    "    stride = hparameters[\"stride\"]\n",
    "    \n",
    "    #获取A_prev和dA的基本信息\n",
    "    (m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape\n",
    "    (m, n_H, n_W, n_C) = dA.shape\n",
    "    \n",
    "    #初始化输出结构\n",
    "    dA_prev = np.zeros_like(A_prev)\n",
    "    \n",
    "    #开始处理数据\n",
    "    for i in range(m):\n",
    "        a_prev = A_prev[i]\n",
    "        for h in range(n_H):\n",
    "            for w in range(n_W):\n",
    "                for c in range(n_C):\n",
    "                    vert_start = h * stride\n",
    "                    vert_end   = vert_start + f\n",
    "                    horiz_start= w * stride\n",
    "                    horiz_end  = horiz_start+ f\n",
    "                    \n",
    "                    if mode == \"max\":\n",
    "                        #切片\n",
    "                        a_prev_slice = a_prev[vert_start:vert_end, horiz_start:horiz_end,c]\n",
    "                        #创建代码\n",
    "                        mask = create_mask_from_window(a_prev_slice)\n",
    "                        #计算dA_prev\n",
    "                        dA_prev[i,vert_start:vert_end, horiz_start:horiz_end,c] += np.multiply(mask, dA[i,h,w,c])\n",
    "                        \n",
    "                    elif mode == \"average\":\n",
    "                        #获取dA的值\n",
    "                        da = dA[i,h,w,c]\n",
    "                        #定义过滤器的大小\n",
    "                        shape = (f,f)\n",
    "                        #平均分配\n",
    "                        dA_prev[i,vert_start:vert_end, horiz_start:horiz_end,c] += distribute_value(da,shape)\n",
    "    \n",
    "    #检验\n",
    "    assert(dA_prev.shape == (m,n_H_prev,n_W_prev,n_C_prev))\n",
    "    \n",
    "    return dA_prev\n",
    "\n",
    "#test\n",
    "# np.random.seed(1)\n",
    "# A_prev = np.random.randn(5, 5, 3, 2)\n",
    "# hparameters = {\"stride\" : 1, \"f\": 2}\n",
    "# A, cache = pool_forward(A_prev, hparameters)\n",
    "# dA = np.random.randn(5, 4, 2, 2)\n",
    "\n",
    "# dA_prev = pool_backward(dA, cache, mode = \"max\")\n",
    "# print(\"mode = max\")\n",
    "# print('mean of dA = ', np.mean(dA))\n",
    "# print('dA_prev[1,1] = ', dA_prev[1,1])  \n",
    "# print()\n",
    "# dA_prev = pool_backward(dA, cache, mode = \"average\")\n",
    "# print(\"mode = average\")\n",
    "# print('mean of dA = ', np.mean(dA))\n",
    "# print('dA_prev[1,1] = ', dA_prev[1,1]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "paperback-surfing",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import tensorflow.compat.v1 as tf \n",
    "from tensorflow.python.framework import ops\n",
    "tf.disable_v2_behavior()\n",
    "\n",
    "import cnn_utils\n",
    "\n",
    "%matplotlib inline\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "extended-italic",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X = Tensor(\"Placeholder:0\", shape=(?, 64, 64, 3), dtype=float32)\n",
      "Y = Tensor(\"Placeholder_1:0\", shape=(?, 6), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "def create_placeholders(n_H0, n_W0, n_C0, n_y):\n",
    "    \"\"\"\n",
    "    为session创建占位符\n",
    "    \n",
    "    参数：\n",
    "        n_H0 - 实数，输入图像的高度\n",
    "        n_W0 - 实数，输入图像的宽度\n",
    "        n_C0 - 实数，输入的通道数\n",
    "        n_y  - 实数，分类数\n",
    "        \n",
    "    输出：\n",
    "        X - 输入数据的占位符，维度为[None, n_H0, n_W0, n_C0]，类型为\"float\"\n",
    "        Y - 输入数据的标签的占位符，维度为[None, n_y]，维度为\"float\"\n",
    "    \"\"\"\n",
    "    X = tf.placeholder(tf.float32,[None,n_H0,n_W0,n_C0])\n",
    "    Y = tf.placeholder(tf.float32,[None,n_y])\n",
    "    \n",
    "    return X,Y\n",
    "\n",
    "#test\n",
    "# X , Y = create_placeholders(64,64,3,6)\n",
    "# print (\"X = \" + str(X))\n",
    "# print (\"Y = \" + str(Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "apparent-petersburg",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 = [ 0.00131723  0.1417614  -0.04434952  0.09197326  0.14984085 -0.03514394\n",
      " -0.06847463  0.05245192]\n",
      "W2 = [-0.08566415  0.17750949  0.11974221  0.16773748 -0.0830943  -0.08058\n",
      " -0.00577033 -0.14643836  0.24162132 -0.05857408 -0.19055021  0.1345228\n",
      " -0.22779644 -0.1601823  -0.16117483 -0.10286498]\n"
     ]
    }
   ],
   "source": [
    "def initialize_parameters():\n",
    "    \"\"\"\n",
    "    初始化权值矩阵，这里我们把权值矩阵硬编码：\n",
    "    W1 : [4, 4, 3, 8]\n",
    "    W2 : [2, 2, 8, 16]\n",
    "    \n",
    "    返回：\n",
    "        包含了tensor类型的W1、W2的字典\n",
    "    \"\"\"\n",
    "    tf.set_random_seed(1)\n",
    "    \n",
    "    W1 = tf.get_variable(\"W1\",[4,4,3,8],initializer=tf.glorot_uniform_initializer(seed=0))\n",
    "    W2 = tf.get_variable(\"W2\",[2,2,8,16],initializer=tf.glorot_uniform_initializer(seed=0))\n",
    "    \n",
    "    parameters = {\"W1\":W1,\n",
    "                  \"W2\":W2}\n",
    "    \n",
    "    return parameters\n",
    "\n",
    "#test\n",
    "# tf.reset_default_graph()\n",
    "# with tf.Session() as sess_test:\n",
    "#     parameters = initialize_parameters()\n",
    "#     init = tf.global_variables_initializer()\n",
    "#     sess_test.run(init)\n",
    "#     print(\"W1 = \" + str(parameters[\"W1\"].eval()[1,1,1]))\n",
    "#     print(\"W2 = \" + str(parameters[\"W2\"].eval()[1,1,1]))\n",
    "    \n",
    "#     sess_test.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "billion-passion",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-44-846bc4624a8a>, line 36)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-44-846bc4624a8a>\"\u001b[1;36m, line \u001b[1;32m36\u001b[0m\n\u001b[1;33m    P = tf.keras.layers.\u001b[0m\n\u001b[1;37m                        ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def forward_propagation(X,parameters):\n",
    "    \"\"\"\n",
    "    实现前向传播\n",
    "    CONV2D -> RELU -> MAXPOOL -> CONV2D -> RELU -> MAXPOOL -> FLATTEN -> FULLYCONNECTED\n",
    "    \n",
    "    参数：\n",
    "        X - 输入数据的placeholder，维度为(输入节点数量，样本数量)\n",
    "        parameters - 包含了“W1”和“W2”的python字典。\n",
    "        \n",
    "    返回：\n",
    "        Z3 - 最后一个LINEAR节点的输出\n",
    "    \n",
    "    \"\"\"\n",
    "    W1 = parameters[\"W1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    \n",
    "    #Conv2d : 步伐: 1, 填充方式为: \"same\"\n",
    "    Z1 = tf.nn.conv2d(X,W1,strides=[1,1,1,1],padding=\"SAME\")\n",
    "    \n",
    "    #RELU:\n",
    "    A1 = tf.nn.relu(Z1)\n",
    "    \n",
    "    #Max pool : 窗口大小：8x8，步伐：8x8，填充方式：“SAME”\n",
    "    P1 = tf.nn.max_pool(A1,ksize=[1,8,8,1],strides=[1,8,8,1],padding=\"SAME\")\n",
    "    \n",
    "    #Conv2d : 步伐：1，填充方式：“SAME”\n",
    "    Z2 = tf.nn.conv2d(P1,W2,strides=[1,1,1,1],padding=\"SAME\")\n",
    "    \n",
    "    #RELU:\n",
    "    A2 = tf.nn.relu(Z2)\n",
    "    \n",
    "    #Max pool : 窗口大小：8x8，步伐：8x8，填充方式：“SAME”\n",
    "    P2 = tf.nn.max_pool(A2,ksize=[1,8,8,1],strides=[1,8,8,1],padding=\"SAME\")\n",
    "    \n",
    "    #一维化上一层的输出\n",
    "    #P = P2[:,-1]    \n",
    "    \n",
    "    #全连接层（FC）：使用没有非线性激活函数的全连接层\n",
    "    Z3 = tf.keras.layers.Dense(6, activation = None)(P)\n",
    "    \n",
    "    return Z3\n",
    "\n",
    "#test\n",
    "# tf.reset_default_graph()\n",
    "# np.random.seed(1)\n",
    "\n",
    "# with tf.Session() as sess_test:\n",
    "#     X,Y = create_placeholders(64,64,3,6)\n",
    "#     parameters = initialize_parameters()\n",
    "#     Z3 = forward_propagation(X,parameters)\n",
    "    \n",
    "#     init = tf.global_variables_initializer()\n",
    "#     sess_test.run(init)\n",
    "    \n",
    "#     a = sess_test.run(Z3,{X: np.random.randn(2,64,64,3), Y: np.random.randn(2,6)})\n",
    "#     print(\"Z3 = \" + str(a))\n",
    "    \n",
    "#     sess_test.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "opening-brush",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(Z3,Y):\n",
    "    \"\"\"\n",
    "    计算成本\n",
    "    参数：\n",
    "        Z3 - 正向传播最后一个LINEAR节点的输出，维度为（6，样本数）。\n",
    "        Y - 标签向量的placeholder，和Z3的维度相同\n",
    "    \n",
    "    返回：\n",
    "        cost - 计算后的成本\n",
    "    \n",
    "    \"\"\"\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=Z3,labels=Y))\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "british-behavior",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X_train, Y_train, X_test, Y_test, learning_rate=0.009, \n",
    "         num_epochs=100,minibatch_size=64,print_cost=True,isPlot=True):\n",
    "    \"\"\"\n",
    "    使用TensorFlow实现三层的卷积神经网络\n",
    "    CONV2D -> RELU -> MAXPOOL -> CONV2D -> RELU -> MAXPOOL -> FLATTEN -> FULLYCONNECTED\n",
    "    \n",
    "    参数：\n",
    "        X_train - 训练数据，维度为(None, 64, 64, 3)\n",
    "        Y_train - 训练数据对应的标签，维度为(None, n_y = 6)\n",
    "        X_test - 测试数据，维度为(None, 64, 64, 3)\n",
    "        Y_test - 训练数据对应的标签，维度为(None, n_y = 6)\n",
    "        learning_rate - 学习率\n",
    "        num_epochs - 遍历整个数据集的次数\n",
    "        minibatch_size - 每个小批量数据块的大小\n",
    "        print_cost - 是否打印成本值，每遍历100次整个数据集打印一次\n",
    "        isPlot - 是否绘制图谱\n",
    "        \n",
    "    返回：\n",
    "        train_accuracy - 实数，训练集的准确度\n",
    "        test_accuracy - 实数，测试集的准确度\n",
    "        parameters - 学习后的参数\n",
    "    \"\"\"\n",
    "    ops.reset_default_graph()\n",
    "    tf.set_random_seed(1)\n",
    "    seed = 3\n",
    "    (m, n_H0, n_W0, n_C0) = X_train.shape\n",
    "    n_y = Y_train.shape[1]\n",
    "    costs = []\n",
    "    \n",
    "    #为当前维度创建占位符\n",
    "    X, Y = create_placeholders(n_H0,b_W0,n_C0,n_y)\n",
    "    \n",
    "    #初始化参数\n",
    "    parameters = initialize_parameters()\n",
    "    \n",
    "    #前向传播\n",
    "    Z3 = forward_propagation(X,parameters)\n",
    "    \n",
    "    #计算成本\n",
    "    cost = compute_cost(Z3, Y)\n",
    "    \n",
    "    #反向传播\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "    \n",
    "    #初始化tf变量\n",
    "    init = tf.global_variables_initializer()\n",
    "    \n",
    "    #run\n",
    "    with tf.Session() as sess:\n",
    "        #初始化参数\n",
    "        sess.run(init)\n",
    "        #遍历数据集\n",
    "        for epoch in range(num_epochs):\n",
    "            minibatch_cost = 0\n",
    "            num_minibatches = int(m/minibatch_size)\n",
    "            seed = seed + 1\n",
    "            minibatches = cnn_utils.random_mini_batches(X_train,Y_train,minibatch_size,seed)\n",
    "            \n",
    "            #对每个数据块进行处理\n",
    "            (minibatch_X,minibatch_Y) = minibatch\n",
    "            #最小化这个数据块的成本\n",
    "            _, temp_cost = sess.run([optimizer,cost],feed_dict={X:minibatch_X,Y:minibatch_Y})\n",
    "            \n",
    "            #累加数据块的成本值\n",
    "            minibatch_cost += temp_cost / num_minibatches\n",
    "            \n",
    "            #是否打印成本\n",
    "            if print_cost:\n",
    "                if epoch % 5 == 0:\n",
    "                    print(\"当前是第 \" + str(epoch) + \"代，成本值为:\" + str(minibatch_cost))\n",
    "                    \n",
    "            #记录成本\n",
    "            if epoch % 1 == 0:\n",
    "                costs.append(minibatch_cost)\n",
    "            \n",
    "            #画图\n",
    "            if isPlot:\n",
    "                plt.plot(np.squeeze(costs))\n",
    "                plt.ylabel('cost')\n",
    "                plt.xlabel('iterations (per tens)')\n",
    "                plt.title(\"learning rate=\" + str(learning_rate))\n",
    "                plt.show()\n",
    "                \n",
    "            \n",
    "            #开始预测\n",
    "            ##计算当前预测情况\n",
    "            predict_op = tf.arg_max(Z3,1)\n",
    "            corrent_prediction = tf.equal(predict_op, tf.arg_max(Y,1))\n",
    "            \n",
    "            ##计算精准度\n",
    "            accuracy = tf.reduce_mean(tf.cast(corrent_prediction,\"float\"))\n",
    "            print(\"corrent prediction accuracy= \" + str(accuracy))\n",
    "\n",
    "            train_accuracy = accuracy.eval({X: X_train, Y: Y_train})\n",
    "            test_accuracy  = accuracy.eval({X: X_test,  Y: Y_test})\n",
    "            \n",
    "            print(\"训练集准确度: \" + str(train_accuracy))\n",
    "            print(\"测试集准确度: \" + str(test_accuracy))\n",
    "            \n",
    "            return (train_accuracy,test_accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
